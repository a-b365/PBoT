{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D-fbgBehLbX"
      },
      "source": [
        "## LLamaIndex is the framwork for Context-Augemented LLM Applications.\n",
        "\n",
        "LLamaIndex imposes no restriction on how you use LLMs. You can use LLMs as auto-complete chatbots, agents, and more. It provides tools like:\n",
        "\n",
        "\n",
        "\n",
        "*   Data connectors ingest your existing data from your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\n",
        "*   Data indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\n",
        "*   Engines provide natural language access to your data. Query engines are powerful interfaces for question-answering (e.g. a RAG flow) and Chat engines are conversational messages for multi-message, \"back and forth\" interactions with your data.\n",
        "*   Agents are LLM-powered knowledge workers augemented by tools, from simple helper functions to API integrations and more.\n",
        "*   Observability/Evaluation integrations that enable you to rigourously experiment, evaluate, and monitor your app in a virtuous cycle.\n",
        "*   Workflows allow you to combine all of the above into an event-driven system for flexible than other, graph-based approaches.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkGRGYlZaqyr",
        "outputId": "1783e946-334f-4020-e3c9-d0c6260f5a20"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index --quiet\n",
        "!pip install llama-index-llms-gemini --quiet\n",
        "!pip install -q -U google-generativeai --quiet\n",
        "!pip install llama-index-multi-modal-llms-gemini --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2RE1A-tbY8_"
      },
      "source": [
        "Context Augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of the context-augmentation use case, from prototype to production. These tools provide you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting.\n",
        "\n",
        "The most popular example of context-augmentation is Retrieval-Augmented Generation or RAG, which combines context with LLMs at inference time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_a8KvgQeQBO"
      },
      "source": [
        "Agents are LLM-powered knowledge assistants that use tools to perform tasks like research, data extraction, and more. Agents range from simple question-answering to being able to sense, decide and take actions in order to complete tasks. LlamaIndex provides a framework for building agents including the ability to use RAG pipelines as one of many tools to complete a task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMGIO9argl2q"
      },
      "source": [
        "Workflows are multip-step processes that combine one or more agents, data connectors and other tools to complete a task. They are event driven software that allows you to combine a RAG data sources an multiple agents to create a complex application that can perform a wide variety of tasks with reflection, error-correction, and other hallmarks of advanced LLM applications.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOZgOvb8n3ZK"
      },
      "source": [
        "Since we get rate limit error everytime we use OpenAI closed source models, we switch to open source models such as gemini, llama2 etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "CBMqHR8WmheC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "GOOGLE_API_KEY = \"AIzaSyAHuQtUbzh5DhUm_aNAwX1JC5M0DczVTmY\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\PBoT\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "DCloNjFXtYrq",
        "outputId": "88572660-d50a-4957-b013-8ae3ed064037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audie Murphy was a **United States Army soldier** and **actor**. \n",
            "\n",
            "Here's a breakdown of his accomplishments:\n",
            "\n",
            "* **Military:** He was the most decorated American combat soldier of World War II, receiving 33 awards and citations, including the Medal of Honor. He served in the 3rd Infantry Division and fought in North Africa, Sicily, and France.\n",
            "* **Acting:** After the war, he transitioned to acting, becoming a successful Western film star. He starred in over 40 films, including \"The Red Badge of Courage\" and \"To Hell and Back,\" which was based on his own autobiography.\n",
            "\n",
            "Audie Murphy's story is one of courage, resilience, and talent. He is remembered as a true American hero. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "resp = Gemini().complete(\"Audie Murphy was a\")\n",
        "print(resp.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "NY6CTKIBNZuo",
        "outputId": "f4d2dc5a-2be3-4f9d-91ed-d813e39dc1b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: Ahoy there! To help me suggest the perfect dinner, tell me a bit about what you're in the mood for:\n",
            "\n",
            "* **What kind of cuisine are you craving?** (Italian, Mexican, Asian, etc.)\n",
            "* **What ingredients do you have on hand?** (Any leftovers, fresh produce, etc.)\n",
            "* **How much time do you have to cook?** (Quick and easy, or something more elaborate?)\n",
            "* **Are you cooking for yourself or others?** (If so, any dietary restrictions or preferences?)\n",
            "\n",
            "Once I have a better idea of your preferences, I can suggest some delicious dinner options! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "\n",
        "        ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
        "        ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
        "        ChatMessage(role=\"user\", content=\"Help me decide what to have for dinner.\")\n",
        "\n",
        "]\n",
        "\n",
        "resp = Gemini().chat(messages)\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "BowORbNuX6Kr",
        "outputId": "1c3d531a-3ea1-4a9e-aa52-784a9474bfe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's great that you're a fan of Star Wars! It's a beloved franchise with a huge impact on popular culture. \n",
            "\n",
            "While many people agree that Star Wars is a significant and influential sci-fi film, it's important to remember that \"greatest\" is subjective. Different people have different tastes and preferences. There are many other incredible sci-fi movies out there, each with its own unique strengths and appeal. \n",
            "\n",
            "It's also important to acknowledge that George Lucas, while a visionary filmmaker, is not the only talented person involved in the Star Wars universe. The success of the franchise is due to the contributions of countless actors, writers, directors, artists, and crew members. \n",
            "\n",
            "Ultimately, the best way to appreciate the vast world of sci-fi is to explore different films and find the ones that resonate with you the most. \n",
            "\n",
            "Would you like to discuss some other great sci-fi movies? Or perhaps you'd like to share what you love most about Star Wars? \n"
          ]
        }
      ],
      "source": [
        "llm = Gemini()\n",
        "resp = llm.stream_complete(\"Star Wars is the greatest sci-fi movie of all time. George Lucas is the best...\")\n",
        "for r in resp:\n",
        "    print(r.text, end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "-UEhD5ihY-qD",
        "outputId": "869dadba-211a-42b1-dce7-84cb69353f7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-exp-0801\n",
            "models/gemini-1.5-pro-exp-0827\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0827\n"
          ]
        }
      ],
      "source": [
        "#Using other gemini models\n",
        "import google.generativeai as genai\n",
        "\n",
        "for m in genai.list_models():\n",
        "    if \"generateContent\" in m.supported_generation_methods:\n",
        "        print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "aHgWl04kZbDU",
        "outputId": "f4694f45-167c-42e8-99ce-fc00ce6a3f71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Star Wars is a popular and influential science fiction franchise created by George Lucas. The first film in the series, Star Wars: Episode IV – A New Hope, was released in 1977 and became a worldwide phenomenon. The franchise has since grown to include eleven films, as well as numerous television series, books, comics, and video games.\n",
            "\n",
            "Star Wars has been praised for its innovative special effects, exciting action sequences, and memorable characters. However, it has also been criticized for its simplistic plot and lack of character development.\n",
            "\n",
            "Whether or not Star Wars is the greatest sci-fi movie of all time is a matter of opinion. However, there is no doubt that it is one of the most popular and influential films in the history of cinema.\n",
            "\n",
            "George Lucas is an American film director, producer, screenwriter, and entrepreneur. He is best known for creating the Star Wars and Indiana Jones franchises. Lucas has been praised for his innovative filmmaking techniques and his ability to create memorable characters and stories. However, he has also been criticized for his lack of attention to detail and his tendency to micromanage his projects.\n",
            "\n",
            "Whether or not George Lucas is the best filmmaker of all time is a matter of opinion. However, there is no doubt that he is one of the most successful and influential filmmakers in the history of cinema.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "llm = Gemini(model=\"models/gemini-pro\")\n",
        "resp = llm.complete(\"Star Wars is the greatest sci-fi movie of all time. George Lucas is the best...\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "ta1L5QqPZnp2",
        "outputId": "f4a6ec90-89bb-45be-c902-4d04211e6cf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's great that you're a fan of Star Wars! It's a beloved franchise with a huge impact on popular culture. \n",
            "\n",
            "While many people agree that Star Wars is a fantastic sci-fi movie, it's important to remember that opinions on \"greatest\" are subjective. There are many other incredible sci-fi movies out there, each with its own unique strengths and appeal. \n",
            "\n",
            "It's also important to acknowledge that George Lucas, while a visionary filmmaker, has also been criticized for certain aspects of his work. \n",
            "\n",
            "Ultimately, the best way to enjoy movies is to explore different genres and directors, and find what resonates with you personally. \n",
            "\n",
            "Would you like to discuss some other great sci-fi movies, or perhaps explore some of the criticisms of Star Wars? I'm happy to chat about it! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "llm = Gemini()\n",
        "resp = await llm.acomplete(\"Star Wars is the greatest sci-fi movie of all time. George Lucas is the best...\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ltUFIv32aNLg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#Using HuggingFace LLMs\n",
        "\n",
        "%pip install llama-index-llms-huggingface --quiet\n",
        "%pip install llama-index-llms-huggingface-api --quiet\n",
        "!pip install \"transformers[torch]\" \"huggingface_hub[inference]\" --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9lAD7dr1dn6v"
      },
      "outputs": [],
      "source": [
        "#search instruct model, recommend model etc. to know more about them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "T79Ekl5HbB5B"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\PBoT\\.venv\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "d:\\PBoT\\.venv\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "d:\\PBoT\\.venv\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "d:\\PBoT\\.venv\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "d:\\PBoT\\.venv\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import List, Optional\n",
        "\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "HF_TOKEN:Optional[str] = os.environ.get(\"HF_TOKEN\")\n",
        "# locally_run = HuggingFaceLLM(model_name=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "remotely_run = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=HF_TOKEN)\n",
        "# remotely_run_recommend = HuggingFaceInferenceAPI(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY3M5GKRfMQD",
        "outputId": "e435c3f9-dc41-42f0-f9d2-b52e184ec131"
      },
      "outputs": [],
      "source": [
        "completion_response = remotely_run.complete(\"Audie Murphy\")\n",
        "print(completion_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a RAG\n",
        "# Loading using SImple Data Directory\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader, Settings, TreeIndex\n",
        "\n",
        "cv = SimpleDirectoryReader(\"../data\").load_data()\n",
        "\n",
        "new_index = TreeIndex.from_documents(cv)\n",
        "\n",
        "# Setup embeddings and large language model\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "\n",
        "    model_name=\"models/embeddings-001\", api_key=GOOGLE_API_KEY\n",
        "\n",
        ")\n",
        "\n",
        "Settings.llm = Gemini(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "nodes = Settings.node_parser.get_nodes_from_documents(cv)\n",
        "\n",
        "# #Define Storage Context and Store Data\n",
        "\n",
        "# from llama_index.core import StorageContext\n",
        "\n",
        "# #The storage Storage context offers deault document storage for storing the vector embeddings of the data and keeps the data in-memory. allowing it to be indexed later.\n",
        "\n",
        "# storage_context = StorageContext.from_defaults()\n",
        "# storage_context.docstore.add_documents(nodes)\n",
        "\n",
        "# from llama_index.core import SimpleKeywordTableIndex, VectorStoreIndex\n",
        "\n",
        "# vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "# keyword_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)\n",
        "\n",
        "# # Construct Custom Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alphaelite10@gmail.com \n",
            "\n"
          ]
        }
      ],
      "source": [
        "query_engine = new_index.as_query_engine()\n",
        "\n",
        "response = query_engine.query(\"What is my email address?\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_index.storage_context.persist(\"../storage\") # Creates a storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from llama_index.core import StorageContext, load_index_from_storage\n",
        "# storage_context = StorageContext.from_defaults(persist_dir=\"../storage\")\n",
        "# index = load_index_from_storage(storage_context)\n",
        "\n",
        "#Instead of QA create a chatbot\n",
        "# query_engine = index.as_chat_engine()\n",
        "# response = query_engine.chat(\"What was Amir doing during the year 2023?\")\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You participated in a hackathon called KU Hackfest and worked on a project called Crime Detection and Reporting.\n"
          ]
        }
      ],
      "source": [
        "chat_engine = index.as_chat_engine()\n",
        "response = chat_engine.chat(\"What was a single thing i did in 2023?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The user completed the Machine Learning Specialization program in 2023.\n"
          ]
        }
      ],
      "source": [
        "response = chat_engine.chat(\"What else did he do during that time?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: elevenlabs in d:\\pbot\\.venv\\lib\\site-packages (1.9.0)\n",
            "Requirement already satisfied: httpx>=0.21.2 in d:\\pbot\\.venv\\lib\\site-packages (from elevenlabs) (0.27.2)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in d:\\pbot\\.venv\\lib\\site-packages (from elevenlabs) (2.9.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in d:\\pbot\\.venv\\lib\\site-packages (from elevenlabs) (2.23.4)\n",
            "Requirement already satisfied: requests>=2.20 in d:\\pbot\\.venv\\lib\\site-packages (from elevenlabs) (2.32.3)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in d:\\pbot\\.venv\\lib\\site-packages (from elevenlabs) (4.12.2)\n",
            "Requirement already satisfied: websockets>=11.0 in d:\\pbot\\.venv\\lib\\site-packages (from elevenlabs) (13.0.1)\n",
            "Requirement already satisfied: anyio in d:\\pbot\\.venv\\lib\\site-packages (from httpx>=0.21.2->elevenlabs) (4.4.0)\n",
            "Requirement already satisfied: certifi in d:\\pbot\\.venv\\lib\\site-packages (from httpx>=0.21.2->elevenlabs) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\pbot\\.venv\\lib\\site-packages (from httpx>=0.21.2->elevenlabs) (1.0.5)\n",
            "Requirement already satisfied: idna in d:\\pbot\\.venv\\lib\\site-packages (from httpx>=0.21.2->elevenlabs) (3.10)\n",
            "Requirement already satisfied: sniffio in d:\\pbot\\.venv\\lib\\site-packages (from httpx>=0.21.2->elevenlabs) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in d:\\pbot\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->elevenlabs) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\pbot\\.venv\\lib\\site-packages (from pydantic>=1.9.2->elevenlabs) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\pbot\\.venv\\lib\\site-packages (from requests>=2.20->elevenlabs) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pbot\\.venv\\lib\\site-packages (from requests>=2.20->elevenlabs) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\pbot\\.venv\\lib\\site-packages (from anyio->httpx>=0.21.2->elevenlabs) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "#Text to speech using LLamaIndex\n",
        "!pip install elevenlabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "# from IPython.display import Markdown, display\n",
        "# from llama_index.tts import ElevenLabsTTS\n",
        "# from IPython.display import Audio\n",
        "\n",
        "# documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "# index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pinecone pinecone-client --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Starting to build in more depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "\n",
        "from llama_index.core import Settings\n",
        "\n",
        "llm = Gemini(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "embed_model = GeminiEmbedding(embed_model=\"models/embeddings-001\")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"PINECONE_API_KEY\"] = \"234de054-c139-4a09-89b5-8b3f87f3ba0f\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "pinecone_client = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pbot-vectordb\n"
          ]
        }
      ],
      "source": [
        "for index in pinecone_client.list_indexes():\n",
        "    print(index[\"name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymongo\n",
        "from pymongo.server_api import ServerApi\n",
        "import urllib.parse\n",
        "username = urllib.parse.quote_plus(\"alphaelite10\")\n",
        "password = urllib.parse.quote_plus(\"ab@mongodb2024\")\n",
        "client = pymongo.MongoClient(\"mongodb+srv://%s:%s@cluster0.0hmur.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"%(username, password), server_api=ServerApi('1'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['admin', 'local']"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.list_database_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = client.pbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gridfs\n",
        "fs = gridfs.GridFS(db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_file(file_path):\n",
        "    with open(file_path, 'rb') as file_data:\n",
        "        file_id = fs.put(file_data, filename=file_path.split('/')[-1])\n",
        "    print(f\"File stored with ID: {file_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File stored with ID: 66f2be29aa2e714fac33bea0\n"
          ]
        }
      ],
      "source": [
        "store_file(\"../data/readme.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: readme.txt, ID: 66f2be29aa2e714fac33bea0\n"
          ]
        }
      ],
      "source": [
        "for file in fs.find():\n",
        "    print(f\"Filename: {file.filename}, ID: {file._id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_id': ObjectId('66f2be29aa2e714fac33bea0'), 'filename': 'readme.txt', 'chunkSize': 261120, 'length': 181, 'uploadDate': datetime.datetime(2024, 9, 24, 13, 27, 6, 32000)}\n"
          ]
        }
      ],
      "source": [
        "for file in fs._files.find():\n",
        "    print(fs.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: readme.txt, ID: 66f2be29aa2e714fac33bea0\n"
          ]
        }
      ],
      "source": [
        "list(db.chunks.find({\"files_id\": \"66f28377aa2e714fac33be94\"}))\n",
        "\n",
        "for file in fs.find():\n",
        "    print(f\"Filename: {file.filename}, ID: {file._id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_file(filename, output_path):\n",
        "    file_data = fs.find_one({\"filename\": filename})\n",
        "    if file_data:\n",
        "        with open(output_path, 'wb') as file_output:\n",
        "            file_output.write(file_data.read())\n",
        "        print(f\"File '{filename}' retrieved and saved as: {output_path}\")\n",
        "    else:\n",
        "        print(f\"No file found with filename: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'readme.txt' retrieved and saved as: ../data/hackfest.txt\n"
          ]
        }
      ],
      "source": [
        "retrieve_file(\"readme.txt\", \"../data/hackfest.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ObjectId('66f2be29aa2e714fac33bea0')"
            ]
          },
          "execution_count": 231,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(fs._files.find({\"filename\":\"readme.txt\"}))[0][\"_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert each file to a usable document for indexing\n",
        "docs = []\n",
        "for file in files:\n",
        "    # Retrieve the file content from fs.chunks using the file's _id\n",
        "    file_data = fs.get(file['_id']).read()\n",
        "    # Convert binary data to text (assuming text files or similar)\n",
        "    file_content = file_data.decode('utf-8', errors='ignore')\n",
        "    \n",
        "    # Create a document with content and metadata (adjust fields as needed)\n",
        "    doc = {\n",
        "        \"filename\": file[\"filename\"],\n",
        "        \"content\": file_content,\n",
        "        # Add any additional fields you'd like to use\n",
        "    }\n",
        "    docs.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.readers.mongodb import SimpleMongoReader\n",
        "\n",
        "reader = SimpleMongoReader(uri=\"mongodb+srv://%s:%s@cluster0.0hmur.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"%(username, password))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = reader.lazy_load_data(db_name=\"pbot\", collection_name=\"fs.files\", field_names=[\"_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SummaryIndex, Document\n",
        "\n",
        "# Convert the documents to the llama_index Document format\n",
        "index_docs = [Document(text=doc[\"content\"]) for doc in docs]\n",
        "\n",
        "# Use Gemini or another LLM to create the index\n",
        "index = SummaryIndex.from_documents(index_docs, llm=llm, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What's next for Crime Detection and Reporting?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The system currently works well for detecting two types of weapons and humans. The next step is to expand the system to detect more weapons, along with human detection and counting to assess the risk of a situation. The model currently relies on human-weapon holding patterns, so the next step is to add pose detection and facial recognition for better identification and intent analysis. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
